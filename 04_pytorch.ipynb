{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOhbiDfSdqFcmu+xRz2J6ZK",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bartolomeoadrian/computational-vision/blob/main/04_pytorch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Configuración"
      ],
      "metadata": {
        "id": "XFubpp64GPcf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "BATCH_SIZE = 64\n",
        "NUM_EPOCHS = 20\n",
        "LEARNING_RATE = 0.001\n",
        "MOMENTUM = 0.9\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\""
      ],
      "metadata": {
        "id": "hPoU7j26GPxc"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1 - Cargar librerías"
      ],
      "metadata": {
        "id": "zOFOVRCYGGeq"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "9czg8fPFDi83"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import torch.optim as optim\n",
        "from PIL import Image\n",
        "from torch.utils.data import DataLoader"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2 - Cargar y normalizar imágenes"
      ],
      "metadata": {
        "id": "vL1GqJyyG-pK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
        "])\n",
        "\n",
        "FROG_CLASS_IDX = 6\n",
        "\n",
        "def binary_target_transform(label: int) -> int:\n",
        "    \"\"\"Map CIFAR‑10 labels to {0: not_frog, 1: frog}.\"\"\"\n",
        "    return 1 if label == FROG_CLASS_IDX else 0\n",
        "\n",
        "\n",
        "trainset = torchvision.datasets.CIFAR10(\n",
        "    root=\"./data\", train=True, download=True, transform=transform, target_transform=binary_target_transform\n",
        ")\n",
        "trainloader = DataLoader(trainset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2)\n",
        "\n",
        "testset = torchvision.datasets.CIFAR10(\n",
        "    root=\"./data\", train=False, download=True, transform=transform, target_transform=binary_target_transform\n",
        ")\n",
        "testloader = DataLoader(testset, batch_size=BATCH_SIZE, shuffle=False, num_workers=2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BCDA-NIGG_J_",
        "outputId": "b18b8c86-27b5-4ac8-b567-21aa38719a81"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 170M/170M [00:10<00:00, 16.1MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3 - Definir una red neuronal convolucional"
      ],
      "metadata": {
        "id": "M6smIjLIHOjn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class FrogNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 6, kernel_size=5)\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        self.conv2 = nn.Conv2d(6, 16, kernel_size=5)\n",
        "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
        "        self.fc2 = nn.Linear(120, 84)\n",
        "        self.fc3 = nn.Linear(84, 2)  # Binary output (frog / not_frog)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(F.relu(self.conv1(x)))\n",
        "        x = self.pool(F.relu(self.conv2(x)))\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "net = FrogNet().to(DEVICE)"
      ],
      "metadata": {
        "id": "43VRbTBcHOs6"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4 - Definir una función de pérdida y un optimizador"
      ],
      "metadata": {
        "id": "oEoA_c5rHYtB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(net.parameters(), lr=LEARNING_RATE, momentum=MOMENTUM)"
      ],
      "metadata": {
        "id": "rdEV5LUIHY-G"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5 - Entrenar a la red"
      ],
      "metadata": {
        "id": "loYcOQu_Hg17"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(NUM_EPOCHS):\n",
        "    running_loss = 0.0\n",
        "    net.train()\n",
        "    for i, (inputs, labels) in enumerate(trainloader, 1):\n",
        "        inputs, labels = inputs.to(DEVICE), labels.to(DEVICE)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = net(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item()\n",
        "        if i % 100 == 0:\n",
        "            print(f\"[Epoch {epoch+1}/{NUM_EPOCHS} | Batch {i}] Loss: {running_loss/100:.4f}\")\n",
        "            running_loss = 0.0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AlosCXlFHhI5",
        "outputId": "f758a450-6c18-4f83-80e4-048c3631c131"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Epoch 1/20 | Batch 100] Loss: 0.1743\n",
            "[Epoch 1/20 | Batch 200] Loss: 0.1807\n",
            "[Epoch 1/20 | Batch 300] Loss: 0.1832\n",
            "[Epoch 1/20 | Batch 400] Loss: 0.1853\n",
            "[Epoch 1/20 | Batch 500] Loss: 0.1750\n",
            "[Epoch 1/20 | Batch 600] Loss: 0.1798\n",
            "[Epoch 1/20 | Batch 700] Loss: 0.1722\n",
            "[Epoch 2/20 | Batch 100] Loss: 0.1763\n",
            "[Epoch 2/20 | Batch 200] Loss: 0.1790\n",
            "[Epoch 2/20 | Batch 300] Loss: 0.1761\n",
            "[Epoch 2/20 | Batch 400] Loss: 0.1666\n",
            "[Epoch 2/20 | Batch 500] Loss: 0.1652\n",
            "[Epoch 2/20 | Batch 600] Loss: 0.1748\n",
            "[Epoch 2/20 | Batch 700] Loss: 0.1708\n",
            "[Epoch 3/20 | Batch 100] Loss: 0.1684\n",
            "[Epoch 3/20 | Batch 200] Loss: 0.1760\n",
            "[Epoch 3/20 | Batch 300] Loss: 0.1756\n",
            "[Epoch 3/20 | Batch 400] Loss: 0.1765\n",
            "[Epoch 3/20 | Batch 500] Loss: 0.1661\n",
            "[Epoch 3/20 | Batch 600] Loss: 0.1595\n",
            "[Epoch 3/20 | Batch 700] Loss: 0.1660\n",
            "[Epoch 4/20 | Batch 100] Loss: 0.1668\n",
            "[Epoch 4/20 | Batch 200] Loss: 0.1610\n",
            "[Epoch 4/20 | Batch 300] Loss: 0.1625\n",
            "[Epoch 4/20 | Batch 400] Loss: 0.1702\n",
            "[Epoch 4/20 | Batch 500] Loss: 0.1688\n",
            "[Epoch 4/20 | Batch 600] Loss: 0.1654\n",
            "[Epoch 4/20 | Batch 700] Loss: 0.1589\n",
            "[Epoch 5/20 | Batch 100] Loss: 0.1598\n",
            "[Epoch 5/20 | Batch 200] Loss: 0.1653\n",
            "[Epoch 5/20 | Batch 300] Loss: 0.1661\n",
            "[Epoch 5/20 | Batch 400] Loss: 0.1460\n",
            "[Epoch 5/20 | Batch 500] Loss: 0.1676\n",
            "[Epoch 5/20 | Batch 600] Loss: 0.1711\n",
            "[Epoch 5/20 | Batch 700] Loss: 0.1645\n",
            "[Epoch 6/20 | Batch 100] Loss: 0.1667\n",
            "[Epoch 6/20 | Batch 200] Loss: 0.1655\n",
            "[Epoch 6/20 | Batch 300] Loss: 0.1557\n",
            "[Epoch 6/20 | Batch 400] Loss: 0.1599\n",
            "[Epoch 6/20 | Batch 500] Loss: 0.1530\n",
            "[Epoch 6/20 | Batch 600] Loss: 0.1567\n",
            "[Epoch 6/20 | Batch 700] Loss: 0.1567\n",
            "[Epoch 7/20 | Batch 100] Loss: 0.1593\n",
            "[Epoch 7/20 | Batch 200] Loss: 0.1570\n",
            "[Epoch 7/20 | Batch 300] Loss: 0.1614\n",
            "[Epoch 7/20 | Batch 400] Loss: 0.1543\n",
            "[Epoch 7/20 | Batch 500] Loss: 0.1512\n",
            "[Epoch 7/20 | Batch 600] Loss: 0.1467\n",
            "[Epoch 7/20 | Batch 700] Loss: 0.1550\n",
            "[Epoch 8/20 | Batch 100] Loss: 0.1508\n",
            "[Epoch 8/20 | Batch 200] Loss: 0.1591\n",
            "[Epoch 8/20 | Batch 300] Loss: 0.1377\n",
            "[Epoch 8/20 | Batch 400] Loss: 0.1430\n",
            "[Epoch 8/20 | Batch 500] Loss: 0.1570\n",
            "[Epoch 8/20 | Batch 600] Loss: 0.1631\n",
            "[Epoch 8/20 | Batch 700] Loss: 0.1527\n",
            "[Epoch 9/20 | Batch 100] Loss: 0.1530\n",
            "[Epoch 9/20 | Batch 200] Loss: 0.1458\n",
            "[Epoch 9/20 | Batch 300] Loss: 0.1485\n",
            "[Epoch 9/20 | Batch 400] Loss: 0.1530\n",
            "[Epoch 9/20 | Batch 500] Loss: 0.1560\n",
            "[Epoch 9/20 | Batch 600] Loss: 0.1552\n",
            "[Epoch 9/20 | Batch 700] Loss: 0.1444\n",
            "[Epoch 10/20 | Batch 100] Loss: 0.1446\n",
            "[Epoch 10/20 | Batch 200] Loss: 0.1571\n",
            "[Epoch 10/20 | Batch 300] Loss: 0.1516\n",
            "[Epoch 10/20 | Batch 400] Loss: 0.1523\n",
            "[Epoch 10/20 | Batch 500] Loss: 0.1546\n",
            "[Epoch 10/20 | Batch 600] Loss: 0.1399\n",
            "[Epoch 10/20 | Batch 700] Loss: 0.1367\n",
            "[Epoch 11/20 | Batch 100] Loss: 0.1486\n",
            "[Epoch 11/20 | Batch 200] Loss: 0.1523\n",
            "[Epoch 11/20 | Batch 300] Loss: 0.1512\n",
            "[Epoch 11/20 | Batch 400] Loss: 0.1459\n",
            "[Epoch 11/20 | Batch 500] Loss: 0.1465\n",
            "[Epoch 11/20 | Batch 600] Loss: 0.1443\n",
            "[Epoch 11/20 | Batch 700] Loss: 0.1285\n",
            "[Epoch 12/20 | Batch 100] Loss: 0.1494\n",
            "[Epoch 12/20 | Batch 200] Loss: 0.1453\n",
            "[Epoch 12/20 | Batch 300] Loss: 0.1516\n",
            "[Epoch 12/20 | Batch 400] Loss: 0.1419\n",
            "[Epoch 12/20 | Batch 500] Loss: 0.1431\n",
            "[Epoch 12/20 | Batch 600] Loss: 0.1367\n",
            "[Epoch 12/20 | Batch 700] Loss: 0.1371\n",
            "[Epoch 13/20 | Batch 100] Loss: 0.1433\n",
            "[Epoch 13/20 | Batch 200] Loss: 0.1404\n",
            "[Epoch 13/20 | Batch 300] Loss: 0.1415\n",
            "[Epoch 13/20 | Batch 400] Loss: 0.1352\n",
            "[Epoch 13/20 | Batch 500] Loss: 0.1509\n",
            "[Epoch 13/20 | Batch 600] Loss: 0.1487\n",
            "[Epoch 13/20 | Batch 700] Loss: 0.1411\n",
            "[Epoch 14/20 | Batch 100] Loss: 0.1332\n",
            "[Epoch 14/20 | Batch 200] Loss: 0.1435\n",
            "[Epoch 14/20 | Batch 300] Loss: 0.1381\n",
            "[Epoch 14/20 | Batch 400] Loss: 0.1395\n",
            "[Epoch 14/20 | Batch 500] Loss: 0.1460\n",
            "[Epoch 14/20 | Batch 600] Loss: 0.1395\n",
            "[Epoch 14/20 | Batch 700] Loss: 0.1310\n",
            "[Epoch 15/20 | Batch 100] Loss: 0.1411\n",
            "[Epoch 15/20 | Batch 200] Loss: 0.1293\n",
            "[Epoch 15/20 | Batch 300] Loss: 0.1384\n",
            "[Epoch 15/20 | Batch 400] Loss: 0.1338\n",
            "[Epoch 15/20 | Batch 500] Loss: 0.1465\n",
            "[Epoch 15/20 | Batch 600] Loss: 0.1392\n",
            "[Epoch 15/20 | Batch 700] Loss: 0.1376\n",
            "[Epoch 16/20 | Batch 100] Loss: 0.1323\n",
            "[Epoch 16/20 | Batch 200] Loss: 0.1363\n",
            "[Epoch 16/20 | Batch 300] Loss: 0.1368\n",
            "[Epoch 16/20 | Batch 400] Loss: 0.1439\n",
            "[Epoch 16/20 | Batch 500] Loss: 0.1358\n",
            "[Epoch 16/20 | Batch 600] Loss: 0.1376\n",
            "[Epoch 16/20 | Batch 700] Loss: 0.1428\n",
            "[Epoch 17/20 | Batch 100] Loss: 0.1330\n",
            "[Epoch 17/20 | Batch 200] Loss: 0.1399\n",
            "[Epoch 17/20 | Batch 300] Loss: 0.1281\n",
            "[Epoch 17/20 | Batch 400] Loss: 0.1336\n",
            "[Epoch 17/20 | Batch 500] Loss: 0.1397\n",
            "[Epoch 17/20 | Batch 600] Loss: 0.1325\n",
            "[Epoch 17/20 | Batch 700] Loss: 0.1363\n",
            "[Epoch 18/20 | Batch 100] Loss: 0.1321\n",
            "[Epoch 18/20 | Batch 200] Loss: 0.1246\n",
            "[Epoch 18/20 | Batch 300] Loss: 0.1343\n",
            "[Epoch 18/20 | Batch 400] Loss: 0.1305\n",
            "[Epoch 18/20 | Batch 500] Loss: 0.1303\n",
            "[Epoch 18/20 | Batch 600] Loss: 0.1412\n",
            "[Epoch 18/20 | Batch 700] Loss: 0.1348\n",
            "[Epoch 19/20 | Batch 100] Loss: 0.1262\n",
            "[Epoch 19/20 | Batch 200] Loss: 0.1359\n",
            "[Epoch 19/20 | Batch 300] Loss: 0.1346\n",
            "[Epoch 19/20 | Batch 400] Loss: 0.1433\n",
            "[Epoch 19/20 | Batch 500] Loss: 0.1299\n",
            "[Epoch 19/20 | Batch 600] Loss: 0.1245\n",
            "[Epoch 19/20 | Batch 700] Loss: 0.1293\n",
            "[Epoch 20/20 | Batch 100] Loss: 0.1234\n",
            "[Epoch 20/20 | Batch 200] Loss: 0.1305\n",
            "[Epoch 20/20 | Batch 300] Loss: 0.1302\n",
            "[Epoch 20/20 | Batch 400] Loss: 0.1260\n",
            "[Epoch 20/20 | Batch 500] Loss: 0.1344\n",
            "[Epoch 20/20 | Batch 600] Loss: 0.1314\n",
            "[Epoch 20/20 | Batch 700] Loss: 0.1365\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 6 - Prueba de la red"
      ],
      "metadata": {
        "id": "HkoGlIRPH2AQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "net.eval()\n",
        "correct, total = 0, 0\n",
        "with torch.no_grad():\n",
        "    for inputs, labels in testloader:\n",
        "        inputs, labels = inputs.to(DEVICE), labels.to(DEVICE)\n",
        "        outputs = net(inputs)\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "print(f\"Test Accuracy: {100 * correct / total:.2f}% ({correct}/{total})\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lnafox-dH2P3",
        "outputId": "125629fc-92bb-4677-d8d8-7405af7c64e1"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy: 94.81% (9481/10000)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 7 - Predicción"
      ],
      "metadata": {
        "id": "0MnxXWKuH8oP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "toads = [\n",
        "    'https://raw.githubusercontent.com/bartolomeoadrian/computational-vision/refs/heads/main/assets/images/toad_1.jpg',\n",
        "    'https://raw.githubusercontent.com/bartolomeoadrian/computational-vision/refs/heads/main/assets/images/toad_2.jpg',\n",
        "    'https://raw.githubusercontent.com/bartolomeoadrian/computational-vision/refs/heads/main/assets/images/toad_3.jpg'\n",
        "]\n",
        "\n",
        "images = []\n",
        "\n",
        "# Download the images\n",
        "for i, image_url in enumerate(toads):\n",
        "  response = requests.get(image_url)\n",
        "  with open(f'toad_{i}.jpg', 'wb') as f:\n",
        "      f.write(response.content)\n",
        "      images.append(f'toad_{i}.jpg')\n",
        "\n",
        "# Prediction function\n",
        "def predict_image(path: str):\n",
        "    img = Image.open(path).convert(\"RGB\")\n",
        "    img = img.resize((32, 32))  # CIFAR‑10 resolution\n",
        "    tensor = transform(img).unsqueeze(0).to(DEVICE)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = net(tensor)\n",
        "        _, pred = torch.max(outputs, 1)\n",
        "\n",
        "    return \"frog\" if pred.item() == 1 else \"not_frog\"\n",
        "\n",
        "# Predict\n",
        "for i, image in enumerate(images):\n",
        "    result = predict_image(image)\n",
        "    print(f'Toad {i} is: {result}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GyfXoTIsH86_",
        "outputId": "e5e33f57-4902-4bf8-8798-8f2ff72a4105"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Toad 0 is: frog\n",
            "Toad 1 is: frog\n",
            "Toad 2 is: not_frog\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Conclusión\n",
        "\n",
        "El modelo fue entrenado con imagenes de ranas, no de sapos, aún asi se puede notar que dependiendo con el grado de epochs que se entrene a la red obtenedremos cierto grado de predicción acertada sobre las tres imagenes de sapos a probar. Para mejorar este modelo, se podría mejorar el dataset para tener imagenes de sapos e incluso tambien realizar un trabajo de segmentación sobre las imagenes a predecir para aislarlas de su fondo y obtener mejores resultados.\n",
        "Se podría tambien modificar la última capa de la CNN para que sea clasificación en vez de booleana, de esta manera podría haber una posibilidad mas grande de obtener los resultados esperados"
      ],
      "metadata": {
        "id": "-tW0ToqcIl0R"
      }
    }
  ]
}